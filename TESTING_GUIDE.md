# Testing Guide for Refactored Runners

## Quick Testing Scripts

### 1. **test_refactoring.py** - Comprehensive Testing
```bash
# Test specific runners
.venv/bin/python test_refactoring.py iris2 seeds wdbc

# Test all available refactored runners
.venv/bin/python test_refactoring.py --all

# Test with different aggregation function
.venv/bin/python test_refactoring.py iris2 --agg entropy

# Verbose output with detailed metrics
.venv/bin/python test_refactoring.py iris2 --verbose

# Continue testing even if one fails
.venv/bin/python test_refactoring.py --all --continue-on-error
```

### 2. **compare_outputs.sh** - Side-by-Side Comparison
```bash
# Compare specific runner
./compare_outputs.sh iris2 sum
./compare_outputs.sh seeds entropy

# See available runners
./compare_outputs.sh
```

## Available Refactored Runners

### ‚úÖ Standard Runners (Tested & Working)
- **iris2** - Iris dataset with 2x2 matrices
- **seeds** - Seeds dataset with 3x2 matrices  
- **wdbc** - Wisconsin Breast Cancer with 3x10 matrices
- **htru2** - HTRU2 pulsar data with 2x4 matrices
- **penguins** - Penguins dataset with 2x2 matrices

### üîß Custom Preprocessing Runners (Need Testing)
- **wine** - Wine quality with row block aggregation
- **spectroscopy** - Spectroscopy with row blocks
- **abalone** - Abalone with matrix format options (A/B)
- **retail** - Online retail with complex preprocessing

## Testing Results So Far

| Runner | Status | Notes |
|--------|--------|-------|
| iris2  | ‚úÖ PASS | All metrics match within 0.0001 tolerance |
| seeds  | ‚úÖ PASS | All metrics match within 0.0001 tolerance |
| wdbc   | ‚úÖ PASS | All metrics match within 0.0001 tolerance |
| htru2  | ‚è≥ Pending | Should work (standard pattern) |
| penguins | ‚è≥ Pending | Should work (standard pattern) |
| wine   | ‚è≥ Pending | Custom preprocessing needs validation |
| spectroscopy | ‚è≥ Pending | Custom preprocessing needs validation |
| abalone | ‚è≥ Pending | Custom preprocessing needs validation |
| retail | ‚è≥ Pending | Complex preprocessing needs validation |

## What The Tests Verify

1. **Identical Metrics**: All clustering and supervised learning metrics match
2. **Same Data Processing**: Matrix shapes and PBP vector dimensions match
3. **Consistent Outputs**: n_samples, n_features, n_clusters are identical
4. **Error Handling**: Both versions handle edge cases the same way

## Expected Differences (OK)

- **Float Precision**: Refactored versions format to 4 decimal places vs full precision
- **Output Formatting**: Cleaner, more consistent formatting in refactored versions
- **Debug Output Order**: matplotlib messages may appear at different times

## Troubleshooting

### If a test fails:
```bash
# Run with verbose output to see details
.venv/bin/python test_refactoring.py <runner> --verbose

# Check if original runner works first  
.venv/bin/python run_<runner>.py --agg sum --no-plot

# Check if refactored runner works
.venv/bin/python run_<runner>_refactored.py --agg sum --no-plot
```

### Common Issues:
- **Dataset not found**: Make sure data files are downloaded (run original first)
- **Import errors**: Check dataset loader module names
- **Timeout**: Some datasets (wine, retail) may need more time for first download

## Migration Workflow

1. **Test refactored version**: `./compare_outputs.sh <runner>`
2. **Verify metrics match**: Should see "‚úÖ SUCCESS" 
3. **Replace original**: `mv run_<runner>.py run_<runner>_original.py && mv run_<runner>_refactored.py run_<runner>.py`
4. **Update scripts**: Modify any shell scripts that reference the runners

## Next Steps

Once all tests pass:
1. Update `run_top_plots.sh` to use refactored versions
2. Update `CLAUDE.md` with new architecture
3. Consider removing `_refactored` suffix and replacing originals
4. Add unit tests for core modules in `src/`